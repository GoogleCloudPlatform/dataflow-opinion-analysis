{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "5ae57a08-b3c1-4da1-a2a4-f80316323365",
        "_uuid": "963a235008f6f1059420277ede200f813e87ed1d",
        "colab_type": "text",
        "id": "G8vQ713RmNxM"
      },
      "cell_type": "markdown",
      "source": "# Predicting Community Engagement on Reddit"
    },
    {
      "metadata": {
        "_cell_guid": "150ecbe0-719c-473b-b320-c708621045b8",
        "_uuid": "8c2cff0c4f9858f13bbf2bc4f9fb77927054bec8",
        "colab_type": "text",
        "id": "BgEjnpQHZsiy"
      },
      "cell_type": "markdown",
      "source": "This is the companion notebook to the 3-part blog series on the Google Big Data & Machine Learning blog *\"Predicting community engagement on Reddit using TensorFlow, GDELT, and Cloud Dataflow\"* by [datancoffee](https://medium.com/@datancoffee)\n\n* [Part 1](https://cloud.google.com/blog/big-data/2018/03/predicting-community-engagement-on-reddit-using-tensorflow-gdelt-and-cloud-dataflow-part-1)\n* [Part 2](https://cloud.google.com/blog/big-data/2018/03/predicting-community-engagement-on-reddit-using-tensorflow-gdelt-and-cloud-dataflow-part-2)\n* [Part 3](https://cloud.google.com/blog/big-data/2018/03/predicting-community-engagement-on-reddit-using-tensorflow-gdelt-and-cloud-dataflow-part-3)\n\n### Getting Started\n\nYou can run this notebook either in [Colab](https://colab.research.google.com/) or in [Kaggle](https://www.kaggle.com/). \n\n\n#### Running in Colab\n\nSet the following variable to True in the \"Define Constants and Global Variables\" code cell\n\n  `current_run_in_colab=True`\n\n\nDecide if you want to get the training data from the [datancoffee](https://bigquery.cloud.google.com/dataset/datancoffee:discussion_opinions?pli=1) BigQuery dataset or from snapshot CSV files. At present time only Colab allows you accessing the datancoffee BigQuery dataset. To get training data from BigQuery, set the following variable to True in the \"Define Constants and Global Variables\" code cell\n\n  `current_read_from_bq=True`\n\nTo get training data from snapshot files, verify that this variable is set to False . By default it is set to False.\n\n  `current_read_from_bq=False # this is the default value`\n\nPrior to running the notebook, download and setup the snapshot files.\n\n##### Getting the snapshot dataset\n\nDownload the [reddit-ds.zip](https://github.com/GoogleCloudPlatform/dataflow-opinion-analysis/blob/master/models/data/reddit-ds.zip) snapshot file achive from github repository\n\nUnzip the archive and move its contents to the `input_dir` directory. \nBy default, the `input_dir` directory is set to `./tfprojects/results`. This path is relative to where the Jupyter process was started. If you prefer to set an absolute path (e.g. if you are having issues locating this directory), change the `INPUT_DIR_PREFERENCE` variable, and `input_dir` will be adjusted to that location. \nThe archive contains 3 files:\n* reddit-ds-CommentsClassification-IncludeAuto.csv\n* reddit-ds-MlbSubredditClassification-ExcludeAuto.csv\n* reddit-ds-SubredditClassification-ExcludeAuto.csv\n\n#### Running in Kaggle\n\nThis notebook is available as a Kaggle [kernel](https://www.kaggle.com/datancoffee/predicting-community-engagement-on-reddit/).\n\nVerify that the following 2 variables are set to False in the \"Define Constants and Global Variables\" code cell\n\n`current_run_in_colab=False` <br/>\n`current_read_from_bq=False`\n\nThe snapshot [dataset](https://www.kaggle.com/datancoffee/predicting-reddit-community-engagement-dataset) is available in Kaggle, and is packaged with the prediction kernel. You don't have to download and set it up.\n\n\n\n#### Run the model \nExecute all code cells in this notebook, either all at once, or one by one, and observe the various outputs of the model.\n\n\n### Tips and Tricks\n\n#### Run in Kaggle with GPUs\nRunning with GPUs really makes the difference in execution time. Training runs are ~20s vs ~400s with regular CPUs.\n\n#### Improving model accuracy\nThe number of data points in the full Reddit dataset is large, so memory is important. The `current_sample_frac` variable controls the fraction of the input dataset that will be sampled and then divided in training, test and validation subsets. The default settings in the notebook have been selected to run in the publicly hosted versions of Kaggle and Colab. For SubredditClassification goal the setting is current_sample_frac = 0.5 and for the CommentsClassification goal the setting is current_sample_frac = 0.25.\n\nNote that this is about half of what we used in our 3-part blog series.\n\nKaggle gives 6.5GB of memory when running with GPUs. To run the model with more accuracy, self-host the model in an environment with 30-40GB of available memory. In this case you can set current_sample_frac = 0.99 for SubredditClassification and current_sample_frac = 0.5 (or higher) for CommentsClassification.\n\n#### Keras model graphs\nIn Kaggle, to see the Keras model graphs, make sure that the Docker image has the following packages installed: pydot, graphviz. The notebook will handle their absence gracefully, but you won't see the model graphs if they are not installed.\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "64504238-87c2-4b89-8dfc-f96f45389a45",
        "_uuid": "2ab35d4c6491354efdc2c712d442b9a59d5cc7a7",
        "colab_type": "text",
        "id": "PmKLeDScYznR"
      },
      "cell_type": "markdown",
      "source": "## Define Constants and Global Variables"
    },
    {
      "metadata": {
        "_cell_guid": "e9c01df6-9083-4aa4-9a49-89e03d81c349",
        "_uuid": "ad7b7532ce89b6abcd7f81ab7e88890af730f90b",
        "cellView": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "1NM9WQJbYt2d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport collections\nimport tempfile\nimport shutil\nimport time\nimport itertools\nimport os\n\n\nfrom IPython import display\nfrom datetime import datetime\n\n# Will this notebook be run in Colab or Kaggle?\ncurrent_run_in_colab=False\n\n# What is the source of training data? BigQuery or snapshot CSV files\ncurrent_read_from_bq=False\n\n# Fraction of the input dataframe to use for learning (training, validation and test)\ncurrent_sample_frac=0.50 #@param\n# Fraction of the sample to use for test\nTEST_FRAC=0.20 #@param\n\n# What are we trying to do in the model?\n# Options: SubredditClassification, MlbSubredditClassification, CommentsRegression, CommentsClassification\ncurrent_learning_goal = 'SubredditClassification'\n\n# What are our label and feature columns in the input dataset\ncurrent_label_col=''\ncurrent_feature_cols=''\n\n# Special columns in our dataset\nEXAMPLE_WEIGHT_COL = 'ExampleWeight'\nURL_COL = 'Url'\nREDDIT_POSTURL_COL = 'RedditPostUrl'\nURL_LIST_COL = 'UrlList'\nREDDIT_POSTURL_LIST_COL = 'RedditPostUrlList'\n\n# What is the K parameter for embeddings\nEMB_DIM_K = 2\n\n# Should training runs reuse model directory checkpoints, or restart\nRESTART_TRAINING=True #@param\nENABLE_SAVE_TOFILES=False #@param \n\n# Set preferences for outputs, or leave '' empty for defaults\nOUTPUT_DIR_PREFERENCE='./tfprojects' # if set to '' will create subdir under /tmp\nRESULTS_DIR_PREFERENCE='' # if set to '' will create subdir under output_dir\nif current_run_in_colab==True:\n  INPUT_DIR_PREFERENCE='' # if set to '', will set to results_dir\nelse:  \n  INPUT_DIR_PREFERENCE='../input/reddit-ds/' # in Kaggle, files will be available under /input\n\n\n\"\"\"\nOptions for current_label_col: \n  Subreddit or RedditSubmitter (for SubredditClassification), \n  SubredditList (for MlbSubredditClassification),\n  NumCommentersLogScaled (for CommentsRegression), \n  NumCommentersBin, NumCommentsBin, ScoreBin (for CommentsClassification)\n\"\"\"\n\ndef set_columns_for_goal():\n  \n  global current_label_col, current_feature_cols\n  \n  if current_learning_goal=='SubredditClassification':\n    current_label_col = 'Subreddit'\n  elif current_learning_goal=='MlbSubredditClassification':\n    current_label_col = 'SubredditList'\n  elif current_learning_goal=='CommentsRegression':\n    current_label_col = 'NumCommentersLogScaled'\n  elif current_learning_goal=='CommentsClassification':\n    current_label_col = 'ScoreBin'\n  else:\n    current_label_col = 'Subreddit'\n\n\n\n  if current_learning_goal==\"SubredditClassification\":\n    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", EXAMPLE_WEIGHT_COL]\n  elif current_learning_goal==\"MlbSubredditClassification\":\n    current_feature_cols = [URL_LIST_COL, REDDIT_POSTURL_LIST_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitterList\", EXAMPLE_WEIGHT_COL] \n  elif current_learning_goal==\"CommentsRegression\":\n    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", EXAMPLE_WEIGHT_COL]\n  elif current_learning_goal==\"CommentsClassification\":\n    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", \"NumCommentersBin\", \"ScoreBin\", EXAMPLE_WEIGHT_COL]\n  else:\n    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", EXAMPLE_WEIGHT_COL]\n\n  print('Building model with learning goal: %s, using label column: %s, using feature columns: %s' % (current_learning_goal,current_label_col,current_feature_cols))\n\n  return\n\n\n \n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5f97e0f7-a1a4-4086-bb8c-a515a051299a",
        "_uuid": "35ffabd31db1759aa53000cdecd6a1e77f3b829a",
        "colab_type": "text",
        "id": "23UZZozMi_iz"
      },
      "cell_type": "markdown",
      "source": "## Define util functions"
    },
    {
      "metadata": {
        "_cell_guid": "2ea774c0-d81d-4675-a620-303bc87a6042",
        "_uuid": "79cb5b2e6da8f9c233414a1503108f050b43b646",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 88
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 200,
          "status": "ok",
          "timestamp": 1525651228680,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "zLrd3M2cjELf",
        "outputId": "7ff2281a-279e-46f6-bdd0-bca6a196a186",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\"\"\"\nCreate temp directories for outputs\n\"\"\"\n\ndef create_dirs():\n  if OUTPUT_DIR_PREFERENCE=='':\n    output_dir = tempfile.mkdtemp()\n  else:\n    output_dir = OUTPUT_DIR_PREFERENCE\n  #run_id = datetime.fromtimestamp(time.time()).strftime('%Y%m%d-%H%M')  \n  \n  if RESULTS_DIR_PREFERENCE=='':\n    results_dir = os.path.join(output_dir, 'results')\n  else:\n    results_dir = RESULTS_DIR_PREFERENCE\n    \n  if INPUT_DIR_PREFERENCE=='':\n    input_dir = results_dir\n  else:\n    input_dir = INPUT_DIR_PREFERENCE\n    \n  tb_log_dir = clean_tb_log_dir(output_dir)\n  \n  return (output_dir,input_dir,results_dir, tb_log_dir)\n\ndef clean_model_dir():\n  model_dir = os.path.join(output_dir, 'model')\n  if RESTART_TRAINING==True:\n    shutil.rmtree(model_dir, ignore_errors=True)\n    os.makedirs(model_dir)\n  else:\n    os.makedirs(model_dir)\n  return model_dir\n\ndef clean_tb_log_dir(output_dir):\n  tb_log_dir = os.path.join(output_dir, 'tb_log_dir')\n  shutil.rmtree(tb_log_dir, ignore_errors=True)\n  os.makedirs(tb_log_dir)\n  return tb_log_dir\n\n  \n\"\"\"\nDefine some basic file I/O ops\n\"\"\"\n\ndef current_dataset_filename(prefix):\n  res = prefix + '-' + current_learning_goal + '-' + ('ExcludeAuto' if current_exclude_autosubreddits else 'IncludeAuto')\n  return res\n  \n\ndef log_dataframe(df, name):\n  if ENABLE_SAVE_TOFILES:  \n    df.to_csv(os.path.join(results_dir,name+'.csv'), encoding='utf-8', index_label='dataframe_idx')\n\ndef load_raw_dataframe(path, col_names, col_types):\n  # Load it into a pandas dataframe\n  df = pd.read_csv(path, index_col='dataframe_idx', dtype=col_types, header=0).fillna('')\n  print(\"Size of dataframe: \" + str(len(df.index)) + \" records\") \n\n  return df \n\n(output_dir, input_dir, results_dir, tb_log_dir) = create_dirs()\n\nprint(\"Location of output (model etc) files: %s \" % output_dir)\nprint(\"Location of input files: %s \" % input_dir)\nprint(\"Location of Tensorboard log files: %s \" % tb_log_dir)\nprint(\"Location of results files: %s\" % results_dir)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "88c97d86-9de3-4cfe-ba52-c97111fd55a4",
        "_uuid": "37cdeba0e08afdcedf4857d424267298f53cb558",
        "colab_type": "text",
        "id": "qC6s1Lq8a21s"
      },
      "cell_type": "markdown",
      "source": "## Get training data from BigQuery or CSV files"
    },
    {
      "metadata": {
        "_cell_guid": "78a45cb0-74e3-4cf7-b010-8a6a21ad6121",
        "_uuid": "0d1335aefd583a46ad4f1406d9262c91f3a491fe",
        "cellView": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "19_mfqZB5NRl",
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n  from colabtools import bigquery # pylint: disable=g-import-not-at-top\n  PROJECT_ID = 'datancoffee' #@param\n  bqclient = bigquery.Create(project_id=PROJECT_ID) \nexcept ImportError:\n  pass\n\ncurrent_exclude_autosubreddits = True\n\ndef get_snapshot_data_for_goal():\n\n  def get_column_types():\n    \n    if current_learning_goal==\"SubredditClassification\":\n      col_defaults = collections.OrderedDict([\n        (\"dataframe_idx\", [0]),\n        (\"Url\", [\"\"]),\n        (\"RedditPostUrl\", [\"\"]),\n        (\"Domain\", [\"\"]),\n        (\"RedditSubmitter\", [\"\"]),\n        (\"Subreddit\", [\"\"]),\n        (\"Tags\", [\"\"]),\n        (\"BOWEntitiesEncoded\", [\"\"])\n      ])  # pyformat: disable\n    elif current_learning_goal==\"MlbSubredditClassification\":\n      col_defaults = collections.OrderedDict([\n        (\"dataframe_idx\", [0]),\n        (\"DocumentHash\", [\"\"]),\n        (\"Domain\", [\"\"]),\n        (\"Tags\", [\"\"]),\n        (\"BOWEntitiesEncoded\", [\"\"]),\n        (\"UrlList\", [\"\"]),\n        (\"RedditPostUrlList\", [\"\"]),\n        (\"RedditSubmitterList\", [\"\"]),\n        (\"SubredditList\", [\"\"]),\n        (\"Score\", [0]),\n        (\"NumCommenters\", [0]),\n        (\"NumComments\", [0])\n      ])  # pyformat: disable \n    elif current_learning_goal==\"CommentsClassification\":\n      col_defaults = collections.OrderedDict([\n        (\"dataframe_idx\", [0]),\n        (\"Url\", [\"\"]),\n        (\"RedditPostUrl\", [\"\"]),\n        (\"Domain\", [\"\"]),\n        (\"RedditSubmitter\", [\"\"]),\n        (\"Subreddit\", [\"\"]),\n        (\"Tags\", [\"\"]),\n        (\"BOWEntitiesEncoded\", [\"\"]),\n        (\"Score\", [0]),\n        (\"NumCommenters\", [0]),\n        (\"NumComments\", [0])\n      ])  # pyformat: disable    \n    else:\n      col_defaults = collections.OrderedDict([\n        (\"dataframe_idx\", [0])\n      ])  # pyformat: disable     \n\n    \n    col_types = collections.OrderedDict((key, type(value[0]))\n                                    for key, value in col_defaults.items())\n    col_names=col_types.keys()\n        \n    return (col_names, col_types)\n  \n  (col_names, col_types) = get_column_types()\n  \n  filename = current_dataset_filename('reddit-ds')\n  path = os.path.join(input_dir,filename+'.csv')\n  df = load_raw_dataframe(path, col_names, col_types)\n  \n  return df\n\n\ndef get_bq_data_for_goal():\n\n  query = '''\n  WITH \n  s1 AS ( -- Create matches btw reddit and gdelt\n    SELECT \n      nwr.DocumentHash,\n      ARRAY_AGG(DISTINCT nwr.Url) AS MatchedUrls,\n      ARRAY_AGG(DISTINCT nwr.WebResourceHash) AS MatchedNWRs,\n      ARRAY_AGG(DISTINCT rwr.WebResourceHash) AS MatchedRWRs, \n      COUNT(*) AS cnt\n    FROM discussion_opinions.webresource rwr \n      INNER JOIN news_opinions.webresource nwr ON nwr.Url = rwr.MetaFields[SAFE_OFFSET(0)]\n    WHERE \n      rwr.MetaFields[SAFE_OFFSET(0)] <> 'unavailable' -- 0-index metafield contains external URL\n      AND nwr._PARTITIONTIME BETWEEN TIMESTAMP('2017-06-01') AND TIMESTAMP('2017-09-30') \n      AND rwr._PARTITIONTIME BETWEEN TIMESTAMP('2017-06-01') AND TIMESTAMP('2017-09-30') \n    GROUP BY 1\n    ORDER BY 5 DESC\n  )\n  -- SELECT * FROM s1 LIMIT 1000\n  -- SELECT COUNT(*) FROM s1 -- 200265\n  , s2a AS (\n    SELECT nd.DocumentHash, \n      nd.PublicationTime AS NewsPubTime,   \n      nd.Author AS NewsAuthor, \n      ARRAY_AGG(REGEXP_REPLACE(tag.Tag,\"[ | [:punct:]]\",\"_\")) AS TagArray   \n    FROM s1\n      INNER JOIN news_opinions.document nd ON nd.DocumentHash = s1.DocumentHash, UNNEST(nd.Tags) AS tag\n    GROUP BY 1,2,3\n  )\n  , s2b AS (\n    SELECT tag, COUNT(DISTINCT DocumentHash) cnt FROM s2a, UNNEST(s2a.TagArray) AS tag \n    GROUP BY 1 HAVING COUNT(DISTINCT DocumentHash) >= 3\n  )\n  -- SELECT COUNT(*) FROM s2b -- 52264\n  , s2c AS (\n    SELECT DocumentHash, tag2 AS Tag\n    FROM s2a, UNNEST(s2a.TagArray) AS tag2\n      INNER JOIN s2b ON s2b.tag = tag2\n  )\n  , s2 AS (\n    SELECT s2a.DocumentHash, \n      s2a.NewsPubTime,   \n      s2a.NewsAuthor,\n      ARRAY_TO_STRING(ARRAY_AGG(s2c.Tag),\" \") AS Tags\n    FROM s2a\n      LEFT OUTER JOIN s2c ON s2c.DocumentHash = s2a.DocumentHash\n    GROUP BY 1,2,3\n  )\n  -- SELECT COUNT(*) FROM s2 --198657\n  -- SELECT * FROM s2 LIMIT 1000\n  , s3 AS (\n    SELECT s1.DocumentHash, \n      rwr.Author AS RedditSubmitter,\n      rwr.PublicationTime AS RedditPubTime,\n      rwr.MetaFields[SAFE_OFFSET(4)] AS Domain, \n      rwr.MetaFields[SAFE_OFFSET(0)] AS Url,\n      rwr.MetaFields[SAFE_OFFSET(1)] AS Subreddit,\n      rwr.MetaFields[SAFE_OFFSET(2)] AS Score,\n      rwr.CollectionItemId AS RedditPostId\n    FROM s1, UNNEST(s1.MatchedRWRs) AS rwrHash\n      INNER JOIN discussion_opinions.webresource rwr ON rwr.WebResourceHash = rwrHash\n    GROUP BY 1,2,3,4,5,6,7,8\n  ),\n  -- SELECT * FROM s3 LIMIT 1000\n  -- SELECT COUNT(*) FROM s3 -- 429648\n  s3aa AS (\n    SELECT Url FROM s3 GROUP BY 1\n  ),\n  s3ab AS (\n    SELECT gkg.DocumentIdentifier, gkg.V2Themes, gkg.AllNames, gkg.V2Locations\n    FROM `gdelt-bq.gdeltv2.gkg` gkg \n      INNER JOIN s3aa ON s3aa.Url = gkg.DocumentIdentifier\n  )\n  ,s3ac AS ( -- Mentions of Themes\n    SELECT s3ab.DocumentIdentifier, SPLIT(theme_mentions,',')[SAFE_OFFSET(0)] AS Entity, SPLIT(theme_mentions,',')[SAFE_OFFSET(1)] AS Offset\n    FROM s3ab, UNNEST(SPLIT(s3ab.V2Themes,\";\")) AS theme_mentions\n  )\n  -- SELECT * FROM s3ac LIMIT 1000\n  ,s3ad AS (\n    SELECT s3ab.DocumentIdentifier, \n      REPLACE(SPLIT(name_mentions,',')[SAFE_OFFSET(0)],' ','_') AS Name, \n      SPLIT(name_mentions,',')[SAFE_OFFSET(1)] AS Offset\n    FROM s3ab, UNNEST(SPLIT(s3ab.AllNames,\";\")) AS name_mentions\n  )\n  -- SELECT * FROM s3ad LIMIT 1000\n  ,s3ae AS ( -- Calculate frequency stats for Name mentions\n    SELECT Name, COUNT(DISTINCT DocumentIdentifier) FROM s3ad \n    GROUP BY 1 HAVING COUNT(DISTINCT DocumentIdentifier) >= 10\n  )\n  -- SELECT * FROM s3ae LIMIT 1000\n  ,s3af AS (-- Filter mentions of Names\n    SELECT s3ad.DocumentIdentifier, s3ad.Name AS Entity, s3ad.Offset\n    FROM s3ad INNER JOIN s3ae ON s3ae.Name = s3ad.Name\n  )\n  -- SELECT DISTINCT Entity FROM s3af\n  ,s3ag AS ( -- Mentions of Locations\n    SELECT s3ab.DocumentIdentifier, SPLIT(loc_mentions,'#') AS LocFieldArray \n    FROM s3ab, UNNEST(SPLIT(s3ab.V2Locations,\";\")) AS loc_mentions\n  )\n  -- SELECT * FROM s3ag LIMIT 1000\n  ,s3ah AS (\n    SELECT \n      s3ag.DocumentIdentifier, \n      REPLACE(REPLACE(LocFieldArray[SAFE_OFFSET(1)],' ','_'),',','_') AS Loc, \n      LocFieldArray[SAFE_OFFSET(8)] AS Offset\n    FROM s3ag\n  )\n  ,s3ai AS ( -- Calculate frequency stats for Location mentions\n    SELECT Loc, COUNT(DISTINCT DocumentIdentifier) FROM s3ah \n    GROUP BY 1 HAVING COUNT(DISTINCT DocumentIdentifier) >= 10\n  )\n  -- SELECT * FROM s3ae LIMIT 1000\n  ,s3aj AS ( -- Filter mentions of Locations\n    SELECT s3ah.DocumentIdentifier, s3ah.Loc AS Entity, s3ah.Offset\n    FROM s3ah INNER JOIN s3ai ON s3ai.Loc = s3ah.Loc\n  )\n  ,s3ak AS ( -- Join all Themes, Locations, Names\n    SELECT DocumentIdentifier, Entity, Offset FROM s3ac\n    UNION ALL\n    SELECT DocumentIdentifier, Entity, Offset FROM s3af\n    UNION ALL\n    SELECT DocumentIdentifier, Entity, Offset FROM s3aj\n  ) \n  -- SELECT COUNT(DISTINCT Entity) FROM s3ak -- 36412\n  ,s3an AS ( -- Create Encoding for Entities\n    SELECT Entity, cnt, CAST(RANK() OVER (ORDER BY cnt DESC, Entity ASC) AS STRING) AS EntityIdx \n    FROM (SELECT Entity, COUNT(*) AS cnt FROM s3ak GROUP BY 1) \n  )\n  -- SELECT * FROM s3an ORDER BY CAST(EntityIdx AS INT64) ASC LIMIT 1000\n  ,s3a AS (\n    SELECT DocumentIdentifier, \n      STRING_AGG(DISTINCT EntityIdx,\" \") AS BOWEntitiesEncoded, -- For Bag-of-Words encoding order is not important\n      COUNT(DISTINCT s3ak.Entity) AS BOWEncodingLength,\n      STRING_AGG(DISTINCT s3ak.Entity,\" \") AS EntitiesBOW \n      -- STRING_AGG(EntityIdx,\" \" ORDER BY Offset ASC) AS EntitiesSeqEncoded, -- For CNN and RNN analysis, use Entity Sequence\n      -- COUNT(*) AS EntitiesSeqLength,\n      -- STRING_AGG(s3ak.Entity,\" \" ORDER BY Offset ASC) AS EntitiesSeq\n    FROM s3ak\n      INNER JOIN s3an ON s3ak.Entity = s3an.Entity\n    WHERE s3ak.Entity<>\"\"\n    GROUP BY 1\n  )\n  -- SELECT * FROM s3a LIMIT 1000\n  -- SELECT COUNT(*) FROM s3a -- 429648\n  , s3b AS (\n    SELECT s3.RedditPostId, \n      COUNT(DISTINCT rwr.Author) AS NumCommenters,\n      COUNT(*) AS NumComments\n    FROM s3\n      INNER JOIN discussion_opinions.webresource rwr ON rwr.MetaFields[SAFE_OFFSET(3)] = s3.RedditPostId\n    WHERE rwr.Author <> '[deleted]' \n      AND rwr.ParentWebResourceHash IS NOT NULL -- exclude the actual post item\n    GROUP BY 1\n  )\n  -- SELECT * FROM s3b WHERE NumComments < 10 ORDER BY NumCommenters DESC LIMIT 1000\n  -- SELECT COUNT(*) FROM s3b -- 419004\n  , s4 AS (\n    SELECT s2.*, \n      s3.Domain, \n      s3.Url,\n      CONCAT(\"https://www.reddit.com/r/\",s3.Subreddit,\"/comments/\", SUBSTR(s3.RedditPostId,4), \"/\") AS RedditPostUrl,\n      s3.RedditSubmitter,\n      s3.RedditPubTime,\n      s3.Subreddit,\n      s3.Score,\n      IFNULL(s3b.NumCommenters,0) AS NumCommenters,\n      IFNULL(s3b.NumComments,0) AS NumComments,\n      TIMESTAMP_DIFF(s3.RedditPubTime, s2.NewsPubTime,  MINUTE) AS PostSubmitDelay,\n      s3a.BOWEntitiesEncoded,\n      s3a.BOWEncodingLength,\n      s3a.EntitiesBOW\n      -- s3a.EntitiesSeqEncoded,\n      -- s3a.EntitiesSeqLength,\n      -- s3a.EntitiesSeq\n    FROM s2 \n      INNER JOIN s3 ON s3.DocumentHash = s2.DocumentHash\n      LEFT OUTER JOIN s3b ON s3b.RedditPostId = s3.RedditPostId\n      LEFT OUTER JOIN s3a ON s3a.DocumentIdentifier = s3.Url\n  )\n  -- SELECT COUNT(*) FROM s4 -- 425548 / 425548\n  -- SELECT * FROM s4 LIMIT 1000\n  , s5 AS ( -- Creates a ranking of Subreddits based on frequency of posts\n    SELECT Subreddit, cnt, RANK() OVER (ORDER BY cnt DESC) AS SubredditRank\n    FROM (SELECT Subreddit, COUNT(*) AS cnt FROM s4 GROUP BY 1)\n  )\n  , s8 AS (\n    SELECT s4.*\n      , (CASE WHEN s5.SubredditRank < 200 THEN 1 ELSE 0 END) AS IsTop200Subreddit\n    FROM s4\n      INNER JOIN s5 ON s5.Subreddit = s4.Subreddit\n  )\n  , s9 AS (\n    SELECT\n      s8.DocumentHash,\n      s8.RedditPostUrl,\n      s8.Url,\n      s8.Domain,\n      s8.RedditSubmitter AS RedditSubmitter,\n      s8.Subreddit,\n      s8.Score,\n      s8.NumCommenters,\n      s8.NumComments,\n      s8.Tags,\n      IFNULL(s8.BOWEntitiesEncoded,\"\") AS BOWEntitiesEncoded,\n      IFNULL(s8.BOWEncodingLength,0) AS BOWEncodingLength,\n      IFNULL(s8.EntitiesBOW,\"\") AS EntitiesBOW,\n      -- IFNULL(s8.EntitiesSeqEncoded,\"\") AS EntitiesSeqEncoded,\n      -- IFNULL(s8.EntitiesSeqLength,0) AS EntitiesSeqLength,\n      -- IFNULL(s8.EntitiesSeq,\"\") AS EntitiesSeq,\n      (CASE WHEN s8.Subreddit LIKE '%auto' OR s8.Subreddit IN ('AutoNewspaper','UMukhasimAutoNews','newsbotbot','TheNewsFeed') THEN 1 ELSE 0 END) AS IsAutoSubreddit,\n      IsTop200Subreddit\n    FROM s8\n    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n  )\n  '''\n\n\n  if current_learning_goal == 'SubredditClassification':\n    query += '''SELECT Url, RedditPostUrl, Domain, RedditSubmitter, Subreddit, Tags, BOWEntitiesEncoded FROM s9 WHERE IsTop200Subreddit = 1 '''\n    query += ''' AND s9.IsAutoSubreddit = 0 ''' if current_exclude_autosubreddits == True else ''' '''\n      \n  elif current_learning_goal == 'MlbSubredditClassification':\n    query += '''\n  SELECT DocumentHash, Domain, Tags, BOWEntitiesEncoded, \n    STRING_AGG(DISTINCT Url,\" \") AS UrlList,\n    STRING_AGG(DISTINCT RedditPostUrl,\" \") AS RedditPostUrlList,\n    STRING_AGG(DISTINCT RedditSubmitter,\" \") AS RedditSubmitterList, \n    STRING_AGG(DISTINCT Subreddit,\" \") AS SubredditList,\n    MAX(Score) AS Score,\n    SUM(NumCommenters) AS NumCommenters,\n    SUM(NumComments) AS NumComments\n  FROM s9 \n  WHERE IsTop200Subreddit = 1 '''\n    query += ''' AND s9.IsAutoSubreddit = 0 ''' if current_exclude_autosubreddits == True else ''' '''\n    query += '''GROUP BY 1,2,3,4 '''\n  else:\n    query += '''\n    SELECT \n      Url, RedditPostUrl, Domain, RedditSubmitter, Subreddit, Tags, \n      BOWEntitiesEncoded, Score, NumCommenters, NumComments \n    FROM s9 '''\n    query += ''' WHERE s9.IsAutoSubreddit = 0 ''' if current_exclude_autosubreddits == True else ''' '''\n\n\n  df = bigquery.ExecuteQuery(query=query, start_row=0, max_rows = 500000, use_legacy_sql=False)\n  print(\"Size of reddit set: %s records\" % len(df.index)) \n\n  # Dump reddit dataset to CSV for inspection\n  log_dataframe(df,current_dataset_filename('reddit-ds'))\n\n  return df\n\n\ndef get_data_for_goal():\n\n  df = get_bq_data_for_goal() if current_read_from_bq else get_snapshot_data_for_goal()\n  \n  return df\n  \n\nreddit_df = pd.DataFrame()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5fbdaf39-208f-4981-9861-2a2a01361262",
        "_uuid": "6133db189a99d7cb07187d431e62f6a33afacb6f",
        "colab_type": "text",
        "id": "jcxm_vDpmGIB"
      },
      "cell_type": "markdown",
      "source": "\n## Define Transforms of Raw Data into Feature Columns & Input Functions\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "9b61f408-d15d-489f-9fad-0e921a960514",
        "_uuid": "c3b8254bf87ad463cd849d5147c845f32cfad2c6",
        "cellView": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "Bf3M1cDCmtvD",
        "trusted": true
      },
      "cell_type": "code",
      "source": "current_inverse_frequency_pow = 0.75 # used in weight_inverse_to_freq\n\n\ndef get_train_test_sets(fullset_df, train_size, test_size, seed=None):\n  \"\"\"\n  Args:\n    seed: The random seed to use when shuffling the data. `None` generates a\n      unique shuffle every run.\n  Returns:\n    a pair of training data, and test data:\n    `(train, test)`\n  \"\"\"\n\n  # Shuffle the data\n  np.random.seed(seed)\n\n  # Split the data into train/test subsets.\n  sample_size = train_size + test_size\n  sample_df = fullset_df.sample(n=sample_size, random_state=seed)\n  train = sample_df.head(train_size)\n  test = sample_df.drop(train.index)\n\n  return (train, test)\n\n  \n# Split the Dataframe\ndef split_features_labels(raw_df, feature_cols, label_col):\n  \n  features=pd.DataFrame({k: raw_df[k].values for k in feature_cols})\n  labels=pd.Series(raw_df[label_col].values)\n\n  return (features,labels) \n\ndef embedding_dims(num_tokens, k=2):\n  return np.ceil(k * (num_tokens**0.25)).astype(int)\n\n\ndef space_tokenizer_fn(iterator):\n  \n  for x in iterator:\n    if x is not None:\n      yield x.split(\" \")\n    else:\n      yield []\n      \n    \ndef int_converter_fn(a):\n  return np.asarray([int(i) for i in a],dtype=int)\n\n\n\"\"\"\nNormalization utilities compliments of\n  https://github.com/google/eng-edu/blob/master/ml/cc/exercises/improving_neural_net_performance.ipynb\n\"\"\"\ndef linear_scale(series):\n  min_val = series.min()\n  max_val = series.max()\n  scale = (max_val - min_val) / 2.0\n  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n\ndef log1p_normalize(series):\n  return series.apply(lambda x:math.log(x+1.0))\n\ndef log_10_1p_normalize(series):\n  return series.apply(lambda x:math.log10(x+1.0))\n\ndef clip(series, clip_to_min, clip_to_max):\n  return series.apply(lambda x:(\n    min(max(x, clip_to_min), clip_to_max)))\n\ndef z_score_normalize(series):\n  mean = series.mean()\n  std_dv = series.std()\n  return series.apply(lambda x:(x - mean) / std_dv)\n\ndef binary_threshold(series, threshold):\n  return series.apply(lambda x:(1 if x > threshold else 0))\n\n\"\"\"\nLabel Weighting Functions\n\"\"\"\ndef scorebin_weight(series):\n  return series.apply(lambda x:(0.5 if x == \"1\" else 1.0)  )\n\ndef weight_inverse_to_freq(series):\n  \"\"\"\n  Will calculate a weight inverse to the frequency of the label class, \n    making small classes more important than indicated by their frequency.\n    Uses sqrt so as not to diminish the importance of very large classes.\n  \"\"\"\n  val_counts = series.value_counts()\n  min_val = val_counts.min()\n\n  return series.apply(lambda x: ((min_val / float(val_counts[x]))**current_inverse_frequency_pow) )\n\n\ndef add_engineered_columns(df, learning_goal):\n  \n  # rule of thumb, NN's train best when the input features are roughly on the same scale\n  \n  if learning_goal == \"CommentsRegression\" or learning_goal == \"CommentsClassification\":\n    df[\"Score\"] = df[\"Score\"].astype(int)    \n    \n    df[\"NumCommentersClipped\"] = ( clip(df[\"NumCommenters\"],0, (10**4 - 1)) )\n    df[\"NumCommentsClipped\"] = ( clip(df[\"NumComments\"],0, (10**5 - 1)) )\n    df[\"ScoreClipped\"] = ( clip(df[\"Score\"],0, (10**4 - 1)) )\n    \n    df[\"NumCommentersLogScaled\"] = ( log_10_1p_normalize(df[\"NumCommentersClipped\"]) )\n    df[\"NumCommentsLogScaled\"] = ( log_10_1p_normalize(df[\"NumCommentsClipped\"]) )\n    df[\"ScoreLogScaled\"] = ( log_10_1p_normalize(df[\"ScoreClipped\"]) )\n    \n    # This will result in following binning of ScoreBin<-Score: 0: 0, 1: 1-9, 2: 10-99, 3: 100-999, 4: 1000-9999 etc\n    df[\"NumCommentersBin\"] = (np.ceil(df[\"NumCommentersLogScaled\"]).astype(int).astype(str))\n    df[\"NumCommentsBin\"] = (np.ceil(df[\"NumCommentsLogScaled\"]).astype(int).astype(str))\n    df[\"ScoreBin\"] = (np.ceil(df[\"ScoreLogScaled\"]).astype(int).astype(str))\n  \n    df[EXAMPLE_WEIGHT_COL] = weight_inverse_to_freq(df[current_label_col])\n      \n  else:\n    df[EXAMPLE_WEIGHT_COL] = 1.0\n    \n  return\n\ndef convert_series_to_nparray(s):\n  \"\"\"\n  Converts a pandas Series to a numpy array\n  \"\"\"\n  nparray = s.values.astype(type(s[0]))\n  return nparray\n\n\ndef create_train_test_features_labels():\n\n  # Add Engineered Columns\n  add_engineered_columns(reddit_df,learning_goal = current_learning_goal)\n\n\n  # Log results to file\n  log_dataframe(reddit_df,'030-with-engineered-cols')\n\n  reddit_size=len(reddit_df.index)\n  sample_size=int(np.floor(reddit_size * current_sample_frac))\n  test_size=int(np.floor(sample_size * TEST_FRAC))\n  train_size=sample_size - test_size\n\n  (train, test) = get_train_test_sets(fullset_df=reddit_df, train_size=train_size, test_size=test_size,seed=3)\n  print(\"Size of train set: %d records\" %len(train.index)) \n  print(\"Size of test set: %d records\" % len(test.index)) \n\n  # Log results to file\n  log_dataframe(train,'040-train')\n  log_dataframe(test,'050-test')\n\n\n  # Create training and validation splits\n  training_features, training_labels = split_features_labels(raw_df = train, feature_cols=current_feature_cols, label_col=current_label_col)\n  validation_features, validation_labels = split_features_labels(raw_df = test, feature_cols=current_feature_cols, label_col=current_label_col)\n\n  return (training_features, training_labels,validation_features, validation_labels)\n\n\ntraining_features = pd.DataFrame()\ntraining_labels = pd.Series()\nvalidation_features = pd.DataFrame()\nvalidation_labels = pd.Series()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6951dd32-053f-4990-b7ba-6e4478f0bf1c",
        "_uuid": "df50a56ac1045128095d0c7516973fbadaff2075",
        "colab_type": "text",
        "id": "Xd7Lha3VikJp"
      },
      "cell_type": "markdown",
      "source": "## Keras Multi-Label Model"
    },
    {
      "metadata": {
        "_cell_guid": "b95186ab-0fb9-4a0e-834b-4c40eac1d1a3",
        "_uuid": "fe19f5168842f3ca35b0dd1cd86a2c45b354b575",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 34
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 6178,
          "status": "ok",
          "timestamp": 1525651237972,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "dXNlcAFhixiW",
        "outputId": "9168d67f-18cf-4afd-f24c-d3524674ff62",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Keras Multi-Label Model: Tags -> Subreddit\n\nfrom keras.models import Model, Input\nfrom keras.layers import Flatten, Dense, Dropout, Embedding, Activation, LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nimport keras.utils\nfrom sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\nimport bisect\nfrom sklearn import metrics\nfrom keras.utils import plot_model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.metrics import top_k_categorical_accuracy \n\nBATCH_SIZE = 50  \nVALIDATION_SPLIT = 0.1\ncurrent_epochs = 2\n\n# Hidden Layers configuration\nHIDDEN_UNITS_L1 = 500\nDROPOUT_L1 = 0.3\n\nTAGS_MAX_SEQUENCE_LENGTH = 7\nTAGS_MAX_NUM_TOKENS=None # don't limit Tags dictionary \n\nENTITIES_MAX_SEQUENCE_LENGTH = 100\nENTITIES_MAX_NUM_TOKENS=None # don't limit Entities dictionary \n\n\ndef create_bow_input(train, test, max_sequence_length, max_num_tokens=None):\n\n  \"\"\"\n  Args\n    max_num_tokens: if None, will use the entire dictionary of tokens found\n  \"\"\"\n  \n  tokenizer = Tokenizer(num_words=max_num_tokens,filters='')\n  tokenizer.fit_on_texts(train)\n\n  vocab_size = len(tokenizer.word_index) \n  # when creating Embedding layer, we will add 1 to input_dims \n  # to account for the padding 0\n  actual_num_tokens = vocab_size if max_num_tokens == None else min(max_num_tokens,vocab_size)\n\n  train_enc = tokenizer.texts_to_sequences(train)\n  test_enc = tokenizer.texts_to_sequences(test)\n\n  train_enc = pad_sequences(train_enc, maxlen=max_sequence_length, padding='post')\n  test_enc = pad_sequences(test_enc, maxlen=max_sequence_length, padding='post')\n\n  inverted_dict = dict([[v,k] for k,v in tokenizer.word_index.items()])\n  \n  return (train_enc, test_enc, inverted_dict, actual_num_tokens)\n\ndef create_bow_embedded_layer(train, test, feature_key, max_sequence_length, max_num_tokens ):\n  \n  \"\"\"\n    Args:\n      train, test - string columns that need to be encoded with integers\n  \"\"\"\n  \n  (train_enc, test_enc, inverted_dict, actual_num_tokens) = create_bow_input(\n      train=train,\n      test=test,\n      max_sequence_length=max_sequence_length,\n      max_num_tokens=max_num_tokens)\n\n  print(\"Using %d unique values for %s\" % (actual_num_tokens,feature_key))\n\n  # the very first input layer for the feature\n  input_layer = Input(shape=(train_enc.shape[1],), name=feature_key)\n\n  # Embedding layer \n  # When embedding, use (tags_actual_num_tokens +1) to account for the padding 0\n  num_embedding_dims= embedding_dims(actual_num_tokens,EMB_DIM_K)\n  embedding_layer = Embedding(\n      input_dim = (actual_num_tokens +1), # to account for the padding 0\n      output_dim = num_embedding_dims, \n      input_length=max_sequence_length,\n      #mask_zero=True\n      )(input_layer)\n\n  # Adding LSTM layer will work best on sequential data. An LSTM will transform \n  # the vector sequence into a single vector, containing information about the \n  # entire sequence\n  # lstm_layer = LSTM(32)(embedding_layer)\n  # lstm_layer = LSTM(64,return_sequences=True)(embedding_layer)\n\n  # Flatten on top of a Embedding will create a bag-of-words matrix\n  bagofwords_layer = Flatten()(embedding_layer)\n  \n  return (train_enc, test_enc, inverted_dict, actual_num_tokens, input_layer, bagofwords_layer)\n\n\ndef create_categorical_label_or_feature(training_nparray, test_nparray, min_frequency=0, vocab_order=None):\n\n  \"\"\"\n  Use VocabularyProcessor because sklearn LabelEncoder() does not support \n  unseen values in test dataset\n  \"\"\"\n  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n      max_document_length = 1, \n      tokenizer_fn = space_tokenizer_fn,\n      min_frequency=min_frequency\n      #vocabulary=tf.contrib.learn.preprocessing.CategoricalVocabulary()\n  )\n  \n  if vocab_order is not None:\n    vocab_processor.fit(vocab_order)\n  else:\n    vocab_processor.fit(training_nparray)\n    \n  train_enc = vocab_processor.transform(training_nparray)\n  # transform test set using training encoding. Words not found in train set will be set as unknown\n  test_enc = vocab_processor.transform(test_nparray)  \n  \n  train_enc = np.array(list(train_enc))\n  test_enc = np.array(list(test_enc))\n  \n  # VocabularyProcessor outputs a word-id matrix where word ids start from 1 \n  # and 0 means 'no word'. We do not have to subtract 1 from the index, because\n  # keras to_categorical works well with that. We also use 0 to pad sequences.\n  classes = vocab_processor.vocabulary_._reverse_mapping\n  num_classes = len(classes)\n\n  # convert to one-hot representation\n  train_enc = keras.utils.to_categorical(train_enc, num_classes=num_classes) \n  test_enc = keras.utils.to_categorical(test_enc, num_classes=num_classes)\n  \n  return (train_enc, test_enc, num_classes, classes)\n\ndef create_multi_label(training_nparray, test_nparray, max_num_classes=None):\n\n  tokenizer = Tokenizer(num_words=max_num_classes,filters='')\n  tokenizer.fit_on_texts(training_nparray)\n\n  num_classes = len(tokenizer.word_index) + 1 # for the 0 index unknown class\n  actual_num_classes = num_classes if max_num_classes == None else min(max_num_classes,num_classes)\n\n  train_enc = tokenizer.texts_to_matrix(training_nparray)\n  test_enc = tokenizer.texts_to_matrix(test_nparray)\n\n  inverted_dict = dict([[v,k] for k,v in tokenizer.word_index.items()])\n  \n  return (train_enc, test_enc, actual_num_classes, inverted_dict)\n  \n\ndef compile_and_fit_model(inputs = [], outputs=[], use_sample_weights=False, k_of_top_k_accuracy=5):\n  \n  # Prepare output or input: Subreddit\n  if ('Subreddit' in inputs) or ('Subreddit' in outputs):\n    global subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes\n    subreddit_train_nparray = convert_series_to_nparray(training_labels) if ('Subreddit' in outputs) else training_features['Subreddit']\n    subreddit_test_nparray = convert_series_to_nparray(validation_labels) if ('Subreddit' in outputs) else validation_features['Subreddit']\n    \n    if current_learning_goal in ['SubredditClassification','CommentsClassification']:\n      (subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes) = create_categorical_label_or_feature(\n        training_nparray = subreddit_train_nparray,\n        test_nparray = subreddit_test_nparray)  \n\n    elif current_learning_goal==\"MlbSubredditClassification\":\n      (subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes) = create_multi_label(\n        training_nparray = subreddit_train_nparray,\n        test_nparray = subreddit_test_nparray)  \n    print('Using %d unique values for subreddit' % num_subreddit_classes)\n    # Input layer for subreddit\n    if ('Subreddit' in inputs):\n      subreddit_input = Input(shape=(subreddit_train.shape[1],), name='Subreddit')\n\n    \n  # Prepare input: Domain\n  if 'Domain' in inputs:\n    global domain_train, domain_test, num_domain_classes, domain_classes\n    (domain_train, domain_test, num_domain_classes, domain_classes) = create_categorical_label_or_feature(\n        training_nparray = training_features['Domain'],\n        test_nparray = validation_features['Domain'])  \n    print(\"Using %d unique values for domain\" % num_domain_classes)\n    # Input layer for domain\n    domain_input = Input(shape=(domain_train.shape[1],), name='Domain')\n\n  # Prepare input/output: RedditSubmitter\n\n  if ('RedditSubmitter' in inputs) or ('RedditSubmitter' in outputs):\n    global submitter_train, submitter_test, num_submitter_classes, submitter_classes\n    (submitter_train, submitter_test, num_submitter_classes, submitter_classes) = create_categorical_label_or_feature(\n        training_nparray = training_features['RedditSubmitter'],\n        test_nparray = validation_features['RedditSubmitter'],\n        min_frequency = 4)  \n    print(\"Using %d unique values for submitter\" % num_submitter_classes)\n    # Input layer for submitter\n    if ('RedditSubmitter' in inputs):\n      submitter_input = Input(shape=(submitter_train.shape[1],), name='RedditSubmitter')\n\n  # Prepare input: Tags\n\n  #global tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords\n  if 'Tags' in inputs:\n    global tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords\n    (tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords) = create_bow_embedded_layer(\n        train = training_features['Tags'], \n        test = validation_features['Tags'], \n        feature_key = 'Tags', \n        max_sequence_length = TAGS_MAX_SEQUENCE_LENGTH,\n        max_num_tokens = TAGS_MAX_NUM_TOKENS)\n\n  # Prepare input: GDELT Entities \n\n  if 'BOWEntitiesEncoded' in inputs:\n    global entities_train, entities_test, entities_inverted_dict, entities_actual_num_tokens, entities_input, entities_bagofwords\n    (entities_train, entities_test, entities_inverted_dict, entities_actual_num_tokens, entities_input, entities_bagofwords) = create_bow_embedded_layer(\n        train=training_features['BOWEntitiesEncoded'],\n        test=validation_features['BOWEntitiesEncoded'],\n        feature_key = 'BOWEntitiesEncoded',\n        max_sequence_length=ENTITIES_MAX_SEQUENCE_LENGTH,\n        max_num_tokens=ENTITIES_MAX_NUM_TOKENS)\n\n  if ('ScoreBin' in outputs):\n    global scorebin_train, scorebin_test, num_scorebin_classes, scorebin_classes\n    vocab_order = np.sort(training_labels.unique())\n    (scorebin_train, scorebin_test, num_scorebin_classes, scorebin_classes) = create_categorical_label_or_feature(\n        training_nparray = training_features['ScoreBin'],\n        test_nparray = validation_features['ScoreBin'],\n        vocab_order=vocab_order)  \n    print(\"Using %d unique values for ScoreBin\" % num_scorebin_classes)\n\n  if ('NumCommentersBin' in outputs):\n    global numcommentersbin_train, numcommentersbin_test, num_numcommentersbin_classes, numcommentersbin_classes\n    vocab_order = np.sort(training_features['NumCommentersBin'].unique())\n    (numcommentersbin_train, numcommentersbin_test, num_numcommentersbin_classes, numcommentersbin_classes) = create_categorical_label_or_feature(\n        training_nparray = training_features['NumCommentersBin'],\n        test_nparray = validation_features['NumCommentersBin'],\n        vocab_order=vocab_order) \n    print(\"Using %d unique values for NumCommentersBin\" % num_numcommentersbin_classes)\n    \n    \n  ##########################  \n  # Merge input branches and build the model\n  ##########################\n\n  def _create_subreddit_model(model_type='multiclass'):\n    \"\"\"\n      Creates Multi-Class Single-Label model using softmax\n    \"\"\"\n    \n    # Bring together the Inputs\n    input_layers = []\n    preconcat_layers = []\n    for s in inputs:\n      if s=='Tags':\n        input_layers.append(tags_input)\n        preconcat_layers.append(tags_bagofwords)\n      elif s=='BOWEntitiesEncoded':\n        input_layers.append(entities_input)\n        preconcat_layers.append(entities_bagofwords)\n      elif s=='Domain':\n        input_layers.append(domain_input)\n        preconcat_layers.append(domain_input)\n      elif s=='RedditSubmitter':\n        input_layers.append(submitter_input)\n        preconcat_layers.append(submitter_input)\n      elif s=='Subreddit':\n        input_layers.append(subreddit_input)\n        preconcat_layers.append(subreddit_input)\n\n    if len(preconcat_layers) > 1:\n      joined_1 = keras.layers.concatenate(preconcat_layers, axis=-1)\n    elif len(preconcat_layers) == 1:\n      joined_1 = preconcat_layers[0]\n    else:\n      raise ValueError('No valid inputs among %s' % (','.join(inputs)))\n      \n    # Connect to Hidden Layers\n    hidden_1 = Dense(HIDDEN_UNITS_L1, activation='relu')(joined_1)\n    dropout_1 = Dropout(DROPOUT_L1)(hidden_1)\n    \n    if model_type=='multiclass':\n      output_activation='softmax'\n      loss_function = 'categorical_crossentropy'\n    elif model_type=='multilabel':\n      output_activation='sigmoid'\n      loss_function = 'binary_crossentropy'\n    \n    # Add the outputs\n    output_layers = []\n    for s in outputs:\n      if s=='Subreddit':\n        layer = Dense(num_subreddit_classes, activation=output_activation, name='subreddit_output')(dropout_1)\n        output_layers.append(layer)\n      elif s=='RedditSubmitter':\n        layer = Dense(num_submitter_classes, activation=output_activation, name='submitter_output')(dropout_1)\n        output_layers.append(layer)\n      elif s=='ScoreBin':\n        layer = Dense(num_scorebin_classes, activation=output_activation, name='scorebin_output')(dropout_1)\n        output_layers.append(layer)\n      elif s=='NumCommentersBin':\n        layer = Dense(num_numcommentersbin_classes, activation=output_activation, name='numcommentersbin_output')(dropout_1)\n        output_layers.append(layer)\n        \n    model = Model(inputs=input_layers, outputs=output_layers)\n\n    \n    def top_k_accuracy(y_true, y_pred):\n      return top_k_categorical_accuracy(y_true, y_pred, k=k_of_top_k_accuracy)\n    \n    if k_of_top_k_accuracy==5:\n      metrics=['accuracy','top_k_categorical_accuracy']\n    else:\n      metrics=['accuracy',top_k_accuracy]\n    \n    model.compile(optimizer='adam',\n                  loss=loss_function,\n                  # loss_weights=[1., 0.2]\n                  metrics=metrics)\n                 \n\n    return model\n  \n  global x_train, x_test, y_train, y_test\n  \n  if current_learning_goal in ['SubredditClassification','CommentsClassification']:\n    model = _create_subreddit_model(model_type='multiclass')  \n  elif current_learning_goal==\"MlbSubredditClassification\":\n    model = _create_subreddit_model(model_type='multilabel')\n\n  x_train = {}\n  x_test = {}\n  \n  for s in inputs:\n    \n    if s=='Tags':\n      x_train['Tags'] = tags_train\n      x_test['Tags'] = tags_test\n    elif s=='BOWEntitiesEncoded':\n      x_train['BOWEntitiesEncoded'] = entities_train\n      x_test['BOWEntitiesEncoded'] = entities_test\n    elif s=='Domain':\n      x_train['Domain'] = domain_train\n      x_test['Domain'] = domain_test\n    elif s=='RedditSubmitter':\n      x_train['RedditSubmitter'] = submitter_train\n      x_test['RedditSubmitter'] = submitter_test\n    elif s=='Subreddit':\n      x_train['Subreddit'] = subreddit_train\n      x_test['Subreddit'] = subreddit_test\n   \n  y_train = {}\n  y_test = {}\n\n  for s in outputs:\n    \n    if s=='Subreddit':\n      y_train['subreddit_output'] = subreddit_train\n      y_test['subreddit_output'] = subreddit_test\n    elif s=='RedditSubmitter':\n      y_train['submitter_output'] = submitter_train\n      y_test['submitter_output'] = submitter_test\n    elif s=='ScoreBin':\n      y_train['scorebin_output'] = scorebin_train\n      y_test['scorebin_output'] = scorebin_test\n    elif s=='NumCommentersBin':\n      y_train['numcommentersbin_output'] = numcommentersbin_train\n      y_test['numcommentersbin_output'] = numcommentersbin_test\n    \n  \n  \"\"\"\n  callbacks = [\n      keras.callbacks.TensorBoard(\n          log_dir=tb_log_dir,                  \n          histogram_freq=1,                      \n          embeddings_freq=1,                     \n      )\n  ]\n  \"\"\"\n  \n  if use_sample_weights==True:\n    if len(outputs) == 1:\n      sample_weight = np.array(training_features[EXAMPLE_WEIGHT_COL])\n    else:\n      sample_weight = []\n      for s in outputs:\n        sample_weight.append(np.array(training_features[EXAMPLE_WEIGHT_COL]))\n  else:\n    sample_weight = None\n\n  \n  history = model.fit(x_train, y_train, \n            epochs=current_epochs, batch_size=BATCH_SIZE,\n            verbose=2, validation_split=VALIDATION_SPLIT,\n            sample_weight = sample_weight)\n\n  score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n\n  if len(outputs)==1:\n    print('Test data loss: %.3f; top 1 accuracy: %.3f; top %d accuracy: %.3f;'%(score[0],score[1], k_of_top_k_accuracy, score[2]))\n  elif len(outputs)==2:\n    print('Test metrics: total loss: %.3f; output_1 loss: %.3f; output_2 loss: %.3f; output_1 top 1 accuracy: %.3f; output_1 top %d accuracy: %.3f; output_2 top 1 accuracy: %.3f; output_2 top %d accuracy: %.3f;'%(score[0],score[1],score[2],score[3],k_of_top_k_accuracy,score[4],score[5],k_of_top_k_accuracy,score[6]))\n\n  return (model, history, x_train, x_test, y_train, y_test)\n\ndef get_true_and_predicted_labels(model, x_test, y_test_selected, label_classes, multi_output=True, output_idx=0):\n  \n  \"\"\"\n    Run predictions and determine y_true and y_pred filled with labels, not indexes \n  \"\"\"\n  y_pred_probs = model.predict(x_test) # shape: num outputs x num samples x num classes\n  y_pred_probs = np.array(y_pred_probs)\n\n  if multi_output==True:\n    y_pred_idx = np.argmax(y_pred_probs[output_idx],axis=1) \n  else:\n    y_pred_idx = np.argmax(y_pred_probs,axis=1) \n    \n  y_true_idx = np.argmax(y_test_selected,axis=1)\n\n  y_pred = np.array([label_classes[i] for i in y_pred_idx])\n  y_true = np.array([label_classes[i] for i in y_true_idx])\n\n  return (y_true, y_pred)\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "77ea1148-cba2-48f6-a01b-765309008578",
        "_uuid": "e84fb010039afe48cc5af8433e74b486a6f88b41",
        "colab_type": "text",
        "id": "OpFnSPRI83D5"
      },
      "cell_type": "markdown",
      "source": "## Calculate Multi-Label Metrics"
    },
    {
      "metadata": {
        "_cell_guid": "b7ed1a43-f826-4c1c-a28b-8c2e51431da7",
        "_uuid": "181be58e6b5abcddc0b8288cf1cadc94c83d7ef5",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "PXtcYY3RrRLI",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Multi-Label Accuracy calculation based on https://github.com/suraj-deshmukh/Multi-Label-Image-Classification/blob/master/miml.ipynb\n\nfrom sklearn.metrics import matthews_corrcoef, hamming_loss, label_ranking_loss, accuracy_score\nfrom sklearn.metrics import roc_curve, auc\n\nCASE_TYPE_HEADERS = ['100% TP+TN','50%+ TP', '1-49% TP', '0% TP']\n\n\ndef eval_multilabel_metrics(model, x_test, y_true):\n  \"\"\"\n  Returns:\n    y_pred = the matrix of predicted labels\n  \"\"\"\n  y_pred_probs = model.predict(x_test)\n  y_pred_probs = np.array(y_pred_probs)\n\n  ttl_test_samples = y_true.shape[0] \n  num_classes = y_true.shape[1]\n\n\n  threshold = np.arange(0.05,0.95,0.05)\n\n  acc = []\n  accuracies = []\n  best_threshold = np.zeros(y_pred_probs.shape[1])\n  for i in range(y_pred_probs.shape[1]):\n      y_prob = np.array(y_pred_probs[:,i])\n      \n      old_settings = np.seterr(all='ignore') # prevent warnings. matthews_corrcoef handles the NaN case\n      for j in threshold:\n          y_pred = [1 if prob>=j else 0 for prob in y_prob]\n          mcc = matthews_corrcoef(y_true[:,i],y_pred)\n          acc.append(mcc)\n      np.seterr(**old_settings)\n\n      acc   = np.array(acc)\n      index = np.where(acc==acc.max()) \n      accuracies.append(acc.max()) \n      best_threshold[i] = threshold[index[0][0]]\n      acc = []\n\n\n  y_pred = np.array([[1 if y_pred_probs[i,j]>=best_threshold[j] else 0 for j in range(num_classes)] for i in range(ttl_test_samples)])\n\n\n  total_correctly_predicted = len([i for i in range(ttl_test_samples) if (y_true[i]==y_pred[i]).sum() == num_classes])\n  print('Total correctly predicted: %d out of %d (absolute accuracy: %.3f)' % (total_correctly_predicted,ttl_test_samples, total_correctly_predicted/float(ttl_test_samples)))\n\n  acc_score = accuracy_score(y_true,y_pred) #same as above\n  h_loss = hamming_loss(y_true,y_pred)\n  r_loss = label_ranking_loss(y_true,y_pred_probs)\n\n  print('Multi-label accuracy score: %.3f' % acc_score)\n  print('Hamming loss: %.3f' % h_loss)\n  print('Label ranking loss: %.3f' % r_loss)\n\n  return (y_pred, acc_score,h_loss,r_loss)\n\n\n\ndef prettyprint_nparray(nparray, col_headers=None, row_headers=None):\n  df = pd.DataFrame(nparray)\n  if col_headers is not None:\n    df.columns = col_headers\n  if row_headers is not None:\n    df.index = row_headers\n  print(df)\n\ndef get_label_bin_header(bin):\n  \n  l_start = (2 ** max((bin-1),0)) + 1\n  l_end = 2 ** bin\n  if (l_start >= l_end):\n    res = '%d'%l_end\n  else:\n    res = '%d..%d'%(l_start,l_end)  \n  return res\n\ndef gen_label_bin_headers(max_bin):\n  \n  res=[]\n  for i in range(max_bin+1):\n    res.append(get_label_bin_header(i))\n  return res\n    \n    \n\ndef calc_multilabel_accuracy_stats(y_true, y_pred):\n\n  ttl_test_samples = y_true.shape[0]\n  num_classes = y_true.shape[1]\n  \n  max_num_true_labels = max([y_true[i].sum() for i in range(ttl_test_samples)])\n  max_bin = np.ceil(np.log2(max_num_true_labels)).astype(int)\n\n  stats_num_cases = np.zeros((max_bin+1,4),dtype=int)\n  stats_num_samples = np.zeros((max_bin+1),dtype=int)\n  # matrix for indexes in x_test for examples; initialize with np.inf\n  example_by_casetype_bin = np.full((max_bin+1,4),-1,dtype=int) \n\n  for i in range(ttl_test_samples):\n\n    num_true_labels = y_true[i].sum()\n    label_bin = np.ceil(np.log2(num_true_labels)).astype(int)\n\n    num_all_matches = (y_true[i]==y_pred[i]).sum() # 1's and 0's need to match - the most stringent condition\n    num_1_matches = np.array([ 1 if y_true[i][j]==y_pred[i][j] and y_true[i][j] ==1 else 0 for j in range(num_classes) ]).sum() # the 1's match \n\n    if (num_all_matches == num_classes):\n      case_type = 0 # 100% True Positives and 100% True Negatives \n    elif (num_1_matches/float(num_true_labels) >= 0.5):\n      case_type = 1\n    elif (num_1_matches/float(num_true_labels) > 0.0):\n      case_type = 2\n    else:\n      case_type = 3\n\n    stats_num_samples[label_bin] += 1\n    stats_num_cases[label_bin,case_type] += 1\n    \n    if example_by_casetype_bin[label_bin,case_type] == -1:\n      example_by_casetype_bin[label_bin,case_type] = i\n\n  stats_num_cases_ratios = stats_num_cases.astype(float)\n\n  for i in range(stats_num_cases_ratios.shape[0]):\n    # stats_num_cases_ratios[i] = stats_num_cases_ratios[i] / stats_num_samples[i].astype(float)\n    stats_num_cases_ratios[i] = stats_num_cases_ratios[i] / ttl_test_samples\n\n  stats_num_cases_ratios = np.around(stats_num_cases_ratios, decimals=3)  \n  stats_by_casetype = np.sum(stats_num_cases_ratios,0)\n\n  return (stats_num_samples, stats_num_cases, stats_num_cases_ratios, stats_by_casetype, example_by_casetype_bin)\n\n\n\n\ndef get_input_values_at(npdict, idx):\n  \n  \"\"\"\n    Args\n      npdict: dictionary of numpy arrays representing  train or test datasets.\n        used in multi-input keras functional models\n    Returns \n      result_list_nparrays: list of single-element numpy arrays populated by \n        values from npdict located at the same 'idx' position. \n        Can be used for model.predict calls on a single data point\n      result_dict_nparrays: dictionary of single-element numpy arrays\n      \n  \"\"\"\n  result_list_nparrays = [] # TODO: remove the nparray output, as it can cause errors during predictions if keys are sorted differently\n  result_dict_nparrays = {} # the dictionary preserves the key\n  for k,v in npdict.items():\n    v_at_idx = np.array([v[idx]])\n    result_list_nparrays.append( v_at_idx )\n    result_dict_nparrays[k] = v_at_idx\n  return (result_list_nparrays,result_dict_nparrays)\n\n\ndef decode_domain(domain_enc_nparray):\n  \n  idx = np.argmax(domain_enc_nparray)\n  domain = domain_classes[idx]\n  \n  return domain\n\ndef decode_submitter(submitter_enc_nparray):\n  \n  idx = np.argmax(submitter_enc_nparray)\n  submitter = submitter_classes[idx]\n  \n  return submitter\n\ndef decode_tags(tags_enc, show_unknown=False):\n  \n  \"\"\"\n    Args:\n      tags_enc - numpy array with just 1 row\n  \"\"\"\n  if show_unknown:\n    tags_class = lambda idx: '<unknown>' if idx==0 else tags_inverted_dict[idx]\n  else:\n    tags_class = lambda idx: '' if idx==0 else tags_inverted_dict[idx]\n    \n  #tags_decoded = np.array([tags_class(tags_enc[0,idx]) for idx in range(tags_enc.shape[1])])\n  #tags_str = np.array_str(tags_decoded)\n  \n  tags_str = ''\n  \n  for idx in range(tags_enc.shape[1]):\n    tag = tags_class(tags_enc[0,idx])\n    tags_str = ''.join([tags_str, ' ' + tag])\n  \n  return tags_str\n\ndef decode_single_input(single_input_dict):\n  \n  single_input_str=''\n  \n  for k,v in single_input_dict.items():\n    single_feature_str=''\n    \n    if k=='Tags':\n      single_feature_str = decode_tags(v)\n    elif k=='Domain':\n      single_feature_str = decode_domain(v)\n    elif k=='RedditSubmitter':\n      single_feature_str = decode_submitter(v)\n    else:\n      single_feature_str = 'Decode function not implemented'\n    \n    single_input_str = ''.join([single_input_str, ' %s [%s]' % (k,single_feature_str) ])\n  \n  return single_input_str\n\ndef get_class_indeces(nhot_class_array):\n  class_indeces = np.nonzero(nhot_class_array)[0]\n  return class_indeces\n\ndef get_classes(class_indeces, classes_dict):\n  \n  res=[]\n  for i in range(class_indeces.shape[0]):\n    cl = classes_dict[class_indeces[i]]\n    res.append(cl)\n  return res\n\ndef decode_classes(nhot_class_array, classes_dict):\n  class_indeces = get_class_indeces(nhot_class_array)\n  classes = get_classes(class_indeces, classes_dict)\n  res = ','.join(classes)\n  return res\n\ndef print_singlelabel_prediction_samples(y_true, classes_dict,\n                                        top_prediction_K = 5,\n                                        top1_pred_tofind = 1,\n                                        topK_pred_tofind = 1,\n                                        notintopK_pred_tofind = 1,\n                                        start_idx=0):\n\n  ttl_test_samples = y_true.shape[0]\n  \n  str_top1=''\n  str_topK=''\n  str_notintopK=''\n\n  i = start_idx\n  all_examples_found = False\n  \n  while (not all_examples_found) and i < ttl_test_samples:\n\n    (single_input, single_input_dict) = get_input_values_at(x_test, i) # \n    prediction = model.predict(single_input_dict) # resulting prediction is 2D array with shape (1,1)\n    prediction = prediction.flatten() # convert to 1D array\n    \n    # take K largest elements (might be unsorted)\n    topKidx = np.argpartition(prediction, -top_prediction_K)[-top_prediction_K:] \n    # sort first (will be in asc order), then reverse array\n    topKidx = topKidx[np.argsort(prediction[topKidx])]\n    topKidx = topKidx[::-1]\n\n    str_testcase=''\n    str_testcase = ''.join([str_testcase,'Test record index #%d '%i]) \n\n    pred_input = decode_single_input(single_input_dict)\n\n    str_testcase = ''.join([str_testcase,'Prediction input: %s\\n' % pred_input])\n    url_str = get_url_and_reddit_post(validation_features,i)\n    str_testcase = ''.join([str_testcase,url_str])\n\n                            \n    # what is the actual label?\n    #actual_label_idx = np.argmax(y_true[i])\n    actual_label_idx = get_class_indeces(y_true[i])\n    actual_label_idx = actual_label_idx[0]\n    actual_label = classes_dict[actual_label_idx]\n\n    # is it in top K predictions?\n    found_str = ''\n    num_prediction=top_prediction_K+1 # set it to a +1 value to represent \"not in top K\"\n\n    found_in_topK = np.nonzero(topKidx == actual_label_idx)\n    if (len(found_in_topK[0]) > 0):\n      num_prediction = found_in_topK[0][0] + 1\n      found_str = ('[ #' + str(num_prediction) + ' prediction]')  \n    else:\n      found_str = ('[ not found among ' + str(top_prediction_K) + ' top predictions]')\n\n    # build the string with top K classes and their probabilities \n    pred_str = ''\n    for j in range(top_prediction_K):\n      proba = prediction[topKidx[j]]\n      pred_class = classes_dict[topKidx[j]]\n      pred_str = ''.join([pred_str, pred_class + ' (' + '%.2f'%proba + ') '])\n\n    str_testcase = ''.join([str_testcase,'Top %d predicted labels: %s\\n'%(top_prediction_K,pred_str)]) \n    str_testcase = ''.join([str_testcase,'Actual label: %s %s\\n' % (actual_label,found_str)])\n\n    if num_prediction == 1:\n      if top1_pred_tofind > 0:\n        top1_pred_tofind-=1\n        str_top1 = ''.join([str_top1,str_testcase,'\\n'])\n    elif num_prediction <= 5:\n      if topK_pred_tofind > 0:\n        topK_pred_tofind-=1  \n        str_topK = ''.join([str_topK,str_testcase,'\\n'])\n    else:\n      if notintopK_pred_tofind > 0:\n        notintopK_pred_tofind-=1\n        str_notintopK = ''.join([str_notintopK,str_testcase,'\\n'])\n\n    i += 1\n    all_examples_found = True if (top1_pred_tofind + topK_pred_tofind + notintopK_pred_tofind) == 0 else False\n\n  print('Examples of exact matches (Actual Label = Top 1 Predicted Label):\\n%s'%str_top1)\n  print('Examples of approx. matches (Actual Label in Top %d Predicted Labels):\\n%s'%(top_prediction_K,str_topK))\n  print('Examples of bad matches (Actual Label not in Top %d predictions):\\n%s'%(top_prediction_K,str_notintopK))\n\n  return\n\ndef plot_metrics(history):\n  acc = history.history['acc']\n  val_acc = history.history['val_acc']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n  \n  epochs = range(1, len(acc) + 1)\n  \n  plt.plot(epochs, acc, 'bo', label='Training acc')\n  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n  plt.title('Training and validation accuracy')\n  plt.legend()\n  \n  plt.figure()\n  \n  plt.plot(epochs, loss, 'bo', label='Training loss')\n  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n  plt.title('Training and validation loss')\n  plt.legend()\n  \n  plt.show()\n  return\n\n\ndef get_url_and_reddit_post(df, idx):\n  \n  res_str = ''\n  \n  if URL_COL in df.columns:\n    val = df[URL_COL][idx]\n    res_str = ''.join([res_str, 'News Urls: %s\\n' % val])  \n  elif URL_LIST_COL in df.columns:\n    val = df[URL_LIST_COL][idx]\n    res_str = ''.join([res_str, 'News Urls: %s\\n' % val])  \n      \n  if REDDIT_POSTURL_COL in df.columns:\n    val = df[REDDIT_POSTURL_COL][idx]\n    res_str = ''.join([res_str, 'Reddit Post Urls: %s\\n' % val])  \n  elif REDDIT_POSTURL_LIST_COL in df.columns:\n    val = df[REDDIT_POSTURL_LIST_COL][idx]\n    res_str = ''.join([res_str, 'Reddit Post Urls: %s\\n' % val])  \n  \n  return res_str\n  \n  \n\n\ndef print_prediction_and_input(idx, notes, y_true, y_pred, classes_dict):\n  \n  print('Test record index #%d %s'%(idx,notes))\n  print(get_url_and_reddit_post(validation_features,idx))\n  \n  (single_input, single_input_dict) = get_input_values_at(x_test, idx)  \n  pred_input = decode_single_input(single_input_dict)\n  print('Prediction input: %s' % pred_input)\n\n  predicted_classes = decode_classes(y_pred[idx], classes_dict)\n  print('Predicted classes: %s' % predicted_classes)\n\n  true_classes = decode_classes(y_true[idx], classes_dict)\n  print('True/Actual classes: %s' % true_classes)\n\n\ndef calc_multilabel_precision_recall(y_true, y_pred):\n  summary_df = pd.DataFrame(np.empty(0,  dtype=[('subreddit', 'U'),  \n                                                ('sum_true', 'u4'), ('sum_pred', 'u4'), \n                                                ('sum_TP', 'u4'),   ('recall','f4'),\n                                                ('precision','f4'), ('f1_score','f4')]))\n  ttl_num_samples = y_true.shape[0]\n\n  for i in range(num_subreddit_classes):\n    subreddit = '<unknown>' if i==0 else subreddit_classes[i]\n    summary_df.at[i,'subreddit'] = subreddit\n    summary_df.at[i,'sum_true'] = np.sum(y_true[:, i])\n    summary_df.at[i,'sum_pred'] = np.sum(y_pred[:, i])\n    num_1_matches = (np.array([ 1 if y_true[j][i]==y_pred[j][i] and y_true[j][i] ==1 else 0 for j in range(ttl_num_samples) ])).sum() # the 1's match \n    summary_df.at[i,'sum_TP'] = num_1_matches\n\n  summary_df['recall'] = summary_df['sum_TP'] / summary_df['sum_true'] #https://en.wikipedia.org/wiki/Precision_and_recall\n  summary_df['precision'] = summary_df['sum_TP'] / summary_df['sum_pred'] \n  summary_df['f1_score'] = 2 * summary_df['recall'] * summary_df['precision'] / (summary_df['recall'] + summary_df['precision'])\n  \n  summary_df.set_index('subreddit',inplace=True)\n\n  return summary_df\n\n\ndef plot_confusion_matrix(y_true, y_pred, class_order,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n  \"\"\"\n  This function plots the confusion matrix.\n  Args:\n    y_true and y_pred need to be class labels\n    use class_order to define order in the CM matrix\n    Normalization can be applied by setting `normalize=True`\n    \n  \"\"\"\n  \n  found_classes = np.unique(np.concatenate((np.unique(y_true), np.unique(y_pred))))\n  found_and_ordered = []\n  for s in class_order: # preserve order\n    index = np.where(found_classes==s) \n    numfound = len(index[0])\n    if numfound > 0:\n      found_and_ordered.append(s)\n\n  cm = metrics.confusion_matrix(y_true, y_pred, labels=found_and_ordered)\n    \n  if normalize:\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n  np.set_printoptions(precision=2)\n  \n  num_classes = len(found_and_ordered)\n  show_text = True if num_classes < 21 else False\n  \n  # Plot normalized confusion matrix\n  plt.figure(1,figsize=(8,8))\n  \n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(num_classes)\n  plt.xticks(tick_marks, found_and_ordered, rotation=45)\n  plt.yticks(tick_marks, found_and_ordered)\n    \n  #ax.tick_params(axis=u'both', which=u'both',length=0)\n   \n  if show_text==True:\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')\n  \n  plt.show()\n\ndef plot_roc_curves(y_true, y_pred):\n\n  # Compute ROC curve and ROC area for each class\n  fpr = dict()\n  tpr = dict()\n  roc_auc = dict()\n  for i in range(num_subreddit_classes):\n      fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n      roc_auc[i] = auc(fpr[i], tpr[i])\n\n  # Compute micro-average ROC curve and ROC area\n  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n  plt.figure(figsize=(8,8))\n  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title('Receiver operating characteristic example')\n  plt.legend(loc=\"lower right\")\n  lw = 1\n\n  for i in range(num_subreddit_classes):\n\n    plt.plot(fpr[i], tpr[i], color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[i])\n\n  plt.show()\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dd069671-61e8-404a-a1c4-e3668f10cba1",
        "_uuid": "c08a5bd1fc05a3efb157650b9adc00d15344df85",
        "colab_type": "text",
        "id": "FjDTPSp39hLD"
      },
      "cell_type": "markdown",
      "source": "### Tags to Subreddit Classification"
    },
    {
      "metadata": {
        "_cell_guid": "9eeaf0d7-d3ab-495f-96f6-cdbc27d989e3",
        "_uuid": "7a7eb43535679b9a7a2a1cbe26b05655f41ae31c",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 108
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 1691,
          "status": "ok",
          "timestamp": 1525651242327,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "ck6u5eWcYg1U",
        "outputId": "69b9baee-5e10-402a-b5a1-1943cdba470e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "current_learning_goal = 'SubredditClassification'\nset_columns_for_goal()\nreddit_df = get_data_for_goal()\n(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b14ef9ad-8ba7-46a3-ba02-fdc8072b83dd",
        "_uuid": "fecc4d9df2c05dfe20040e8e9a44e625cc4a3672",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 709
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 49321,
          "status": "ok",
          "timestamp": 1525651291697,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "FjLYRPZYaZIq",
        "outputId": "095affb7-705c-4479-91a1-dc6b3671f268",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Tags -> Subreddit \n# Single-Label Classification\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['Tags'], outputs=['Subreddit'])\n\ntry:\n  SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\nexcept ImportError:\n  print('Unable to import pydot and graphviz.') \n  pass\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "55f558b5-6f79-4683-acae-a224f7af28e5",
        "_uuid": "1b0e73f7cc63d98b7f9c021815ef42ca14999fcb",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 562
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 4096,
          "status": "ok",
          "timestamp": 1525651295885,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "kNIOJZdgUkM3",
        "outputId": "e07a950a-2a6d-4bfc-8669-4f8b12b70fb1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['subreddit_output'],\n    multi_output=False,\n    label_classes=subreddit_classes)\n\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=subreddit_classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fcaa5368-e9f3-4a22-ba4c-0283bd47d311",
        "_uuid": "d0c4da1dba84811bb067cfb168696bcbfb3f3d32",
        "colab_type": "text",
        "id": "7rLH231h92JZ"
      },
      "cell_type": "markdown",
      "source": "### Tags to Subreddit Samples"
    },
    {
      "metadata": {
        "_cell_guid": "6a66b485-7a0c-48c2-9c37-602105941b88",
        "_uuid": "c10f518b9010b7fdddb182f17413030d8f94cefa",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 443
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 150,
          "status": "ok",
          "timestamp": 1525651296223,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "W4ateLd5ztyd",
        "outputId": "f14d6d67-7d97-4ab0-f6b1-d369fd7257c9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], \n                                     classes_dict=subreddit_classes)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "735b4ce9-f604-4170-a3eb-9f0dd5411496",
        "_uuid": "3879d6a202a22ac84177672b0a345b9dcf579d09",
        "colab_type": "text",
        "id": "fj_z6yNK9-sA"
      },
      "cell_type": "markdown",
      "source": "### BOWEntitiesEncoded to Subreddit Classification"
    },
    {
      "metadata": {
        "_cell_guid": "3656c250-9dc6-4c65-b26b-e936293b4a94",
        "_uuid": "c1a9b5c463f152441c802426f51e08983ca8c31c",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 585
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 96588,
          "status": "ok",
          "timestamp": 1525651392946,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "s7MxXCyw6v5h",
        "outputId": "e775a66a-6370-4b24-961a-e34ad4088973",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n# BOWEntitiesEncoded -> Subreddit \n# Single-Label Classification\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded'], outputs=['Subreddit'])\n\n# SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n\nprint_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], classes_dict=subreddit_classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e6e15f89-f079-438a-ae5f-3ba7a4c1257e",
        "_uuid": "b048d0b4b1ee3607829992e11acd00d6de54792d",
        "colab_type": "text",
        "id": "Cgn9JoK3-ICf"
      },
      "cell_type": "markdown",
      "source": "### BOWEntitiesEncoded and Domain to Subreddit Classification"
    },
    {
      "metadata": {
        "_cell_guid": "b1f658e7-1455-48ad-8d3d-cd0db36b6348",
        "_uuid": "78dffbde2b25ec678ab11459f284673452bdf27b",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 918
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 189419,
          "status": "ok",
          "timestamp": 1525651582399,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "RzscRUJd9aSM",
        "outputId": "65a18cb1-8576-4d81-e347-8b1f9f7d4f24",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# BOWEntitiesEncoded, Domain -> Subreddit \n# Single-Label Classification\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit'])\n\ntry:\n  SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\nexcept ImportError:\n  print('Unable to import pydot and graphviz.') \n  pass\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "00f5c140-837d-4e01-91fa-5fc1823833d5",
        "_uuid": "4769dc57c8e893d78016ced6cc2417ecff0dc16f",
        "colab_type": "text",
        "id": "0_psJ6-d-SoQ"
      },
      "cell_type": "markdown",
      "source": "### BOWEntitiesEncoded and Domain to Subreddit Classification Samples"
    },
    {
      "metadata": {
        "_cell_guid": "9e12160d-c12c-45eb-8277-545d1d4265c8",
        "_uuid": "ad2ea68ba446556db07bca0801eba582f0cb1b16",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 443
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 532,
          "status": "ok",
          "timestamp": 1525651582975,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "bNvBG5Pw9e0W",
        "outputId": "100fc720-19bc-4010-a02f-9c7d7633a23e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], \n                                     classes_dict=subreddit_classes,\n                                     start_idx=10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "68190900-1612-4798-8555-a5afd4e7bf20",
        "_uuid": "4ec1c1557fcd58ccb28603942ed22e4dfdd9a60a",
        "colab_type": "text",
        "id": "GYeIGFz1-bvt"
      },
      "cell_type": "markdown",
      "source": "### BOWEntitiesEncoded and Domain to Subreddit and RedditSubmitter Classification"
    },
    {
      "metadata": {
        "_cell_guid": "2d931b09-4ca4-4eea-98aa-4ca252552b68",
        "_uuid": "bf2e4b4c88ae4496e485ec8460e94654cb268d88",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 862
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 242373,
          "status": "ok",
          "timestamp": 1525651825463,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "CtMq7mNe--6L",
        "outputId": "196f4a60-6887-4455-dac9-040a38404ba2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# BOWEntitiesEncoded, Domain -> Subreddit, RedditSubmitter \n# Single-Label Classification\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit','RedditSubmitter'])\n\ntry:\n  SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\nexcept ImportError:\n  print('Unable to import pydot and graphviz.') \n  pass\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4f5badd5-ec85-45bd-ae1d-169d994e31ce",
        "_uuid": "8d2ad011851fce47d1f6c1bee2140c3af499452b",
        "colab_type": "text",
        "id": "I_NVBbt9-h8U"
      },
      "cell_type": "markdown",
      "source": "### Subreddit Multi-Label Classification"
    },
    {
      "metadata": {
        "_cell_guid": "dfc0078c-6cd8-4a21-8298-a3b1ac0db29b",
        "_uuid": "4df5b08626c13732ed267fd4da6447b5a3adaaf9",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 915
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 107759,
          "status": "ok",
          "timestamp": 1525651933260,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "9Hy0PerZBvZ3",
        "outputId": "272ab708-ce78-437f-83b9-de5c9ff0978d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# BOWEntitiesEncoded, Domain -> Subreddit \n# Multi-Label Classification\n\ncurrent_learning_goal = 'MlbSubredditClassification'\nset_columns_for_goal()\nreddit_df = get_data_for_goal()\n(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit'])\n\ntry:\n  SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\nexcept ImportError:\n  print('Unable to import pydot and graphviz.') \n  pass\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bbc9a82b-3209-4cba-aef9-269fb8ab52fe",
        "_uuid": "3f95c11a801e43258d24127e53f03184516d08fb",
        "colab_type": "text",
        "id": "wBPVEnYv-sNH"
      },
      "cell_type": "markdown",
      "source": "### Multi-label accuracy metrics"
    },
    {
      "metadata": {
        "_cell_guid": "90a3470a-2013-471b-b74c-ca6c540f5541",
        "_uuid": "beb6b61b7419f83c1fb5ce130543d9ed2aad01c0",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 88
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 137599,
          "status": "ok",
          "timestamp": 1525652070875,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "GHuGRKKqZ69m",
        "outputId": "9298aa00-ab24-4602-9206-434bebedc44e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Multi-label accuracy metrics\n\n(y_pred,acc_score,h_loss,r_loss) = eval_multilabel_metrics(model, x_test, y_true = y_test['subreddit_output'])\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "aa9f9cd8-d73a-4103-810a-2bdff039cc02",
        "_uuid": "d83b27a94c903ac2d5c7325ff1a5ee208695344d",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 2310
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 6240,
          "status": "ok",
          "timestamp": 1525652077202,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "wH_ncWFOxGDX",
        "outputId": "db731d1b-a022-4e79-b0e8-58e7e6f2fba6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Precision and Recall metrics\n#plot_roc_curves(y_true = y_test['subreddit_output'], y_pred=y_pred)\n\nsummary_df = calc_multilabel_precision_recall(y_true = y_test['subreddit_output'], y_pred = y_pred)\n#pd.set_option('display.max_rows', 1000)\nprint(summary_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "72d3d568-3aa4-4bb8-8daf-b1bf7be7b870",
        "_uuid": "d6bd93aaf24747a26937f0f50c400bff7b6f440e",
        "colab_type": "text",
        "id": "dOIH3fWs-ya-"
      },
      "cell_type": "markdown",
      "source": "### Multi-label accuracy by case type"
    },
    {
      "metadata": {
        "_cell_guid": "3b995e8b-38cd-4331-b449-bf31204b7bfa",
        "_uuid": "187ade5d162eeef6d54381a26a95a2b251964e5f",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 763
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 3978,
          "status": "ok",
          "timestamp": 1525652081257,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "Stb-Yk1e0KRm",
        "outputId": "112d8d00-cabc-4c6c-eea7-034cf94a2250",
        "trusted": true
      },
      "cell_type": "code",
      "source": "(stats_num_samples, stats_num_cases, stats_num_cases_ratios, stats_by_casetype, example_by_casetype_bin) = calc_multilabel_accuracy_stats (\n    y_true = y_test['subreddit_output'], y_pred=y_pred)\n\n\nprint('Samples by Number of Labels (2^x scale)')\nprettyprint_nparray(stats_num_samples,\n                    col_headers=['Num Labels'],\n                    row_headers=gen_label_bin_headers(stats_num_samples.shape[0]-1))\n\nprint('\\nSamples by Number of Labels X Case Type')\nprettyprint_nparray(stats_num_cases,\n                    col_headers=CASE_TYPE_HEADERS,\n                    row_headers=gen_label_bin_headers(stats_num_cases.shape[0]-1))\n                     \nprint('\\nSamples by Number of Labels X Case Type (Ratios add up to 100%)')\nprettyprint_nparray(stats_num_cases_ratios,\n                    col_headers=CASE_TYPE_HEADERS,\n                    row_headers=gen_label_bin_headers(stats_num_cases_ratios.shape[0]-1))\n\nprint('\\nSamples by Case Type (Ratios add up to 100%)')\nprettyprint_nparray(stats_by_casetype,\n                    col_headers=['Ratio'],\n                    row_headers=CASE_TYPE_HEADERS)\n\nprint('\\nSample indeces by Number of Labels X Case Type')\nprettyprint_nparray(example_by_casetype_bin,\n                    col_headers=CASE_TYPE_HEADERS,\n                    row_headers=gen_label_bin_headers(example_by_casetype_bin.shape[0]-1))\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "70bb4f2a-da46-428a-bafb-ca3ac7e32739",
        "_uuid": "82f7584ecf01b55e5057b424d2bcba400e9151ef",
        "colab_type": "text",
        "id": "KXeYVumn-82A"
      },
      "cell_type": "markdown",
      "source": "### Multi-Label prediction examples"
    },
    {
      "metadata": {
        "_cell_guid": "85ad90b8-887d-4c83-868c-a1025630f765",
        "_uuid": "ea4d950f89a4454567430a85f0e5e2e30fdcc798",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 283
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 164,
          "status": "ok",
          "timestamp": 1525652081471,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "H25yychh6AFn",
        "outputId": "c9406cbb-d7cd-446c-895a-8e0c3723d8e1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nprint_prediction_and_input(4666, notes='example of 100% TP+TN',\n                           y_true=y_test['subreddit_output'], y_pred=y_pred, classes_dict=subreddit_classes)  \n\nprint('')\n\nprint_prediction_and_input(55, notes='example of 50%+ TP',\n                           y_true=y_test['subreddit_output'], y_pred=y_pred, classes_dict=subreddit_classes)  \n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3d2475d0-82f3-49e3-a2ca-215ade2b40df",
        "_uuid": "9451e496df944ad8c3fa73b2ff6a8e590949acac",
        "colab_type": "text",
        "id": "-OE5bQo8_NFn"
      },
      "cell_type": "markdown",
      "source": "### Adding Submitter as Predictor of Subreddit"
    },
    {
      "metadata": {
        "_cell_guid": "97fd7415-0680-49c5-a10d-d6f694d4edb8",
        "_uuid": "29942a7939be99ea2732ccbcc1abf05ff50321c0",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 933
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 240796,
          "status": "ok",
          "timestamp": 1525652322352,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "9o2ObHjfIZqt",
        "outputId": "a2c8b1d8-66f5-403f-9f5f-eea92f4223fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# BOWEntitiesEncoded, Domain, RedditSubmitter -> Subreddit\n# Single-label classification\n\ncurrent_learning_goal = 'SubredditClassification'\nset_columns_for_goal()\nreddit_df = get_data_for_goal()\n(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain','RedditSubmitter'], outputs=['Subreddit'])\n\ntry:\n  SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\nexcept ImportError:\n  print('Unable to import pydot and graphviz.') \n  pass",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c56645c7-cd22-485c-a0f2-3c5a5064fa72",
        "_uuid": "62d4ddea6a7627805432378f3547818392f9c47c",
        "colab_type": "text",
        "id": "tKnn4WGILlcA"
      },
      "cell_type": "markdown",
      "source": "##Predicting engagement metrics (Score, Number of Commenters, Number of Comments)"
    },
    {
      "metadata": {
        "_cell_guid": "3d365df4-6f07-4402-8fca-1a7d1cb11d52",
        "_uuid": "cb298c9665f18e718357aefd633e3e325e87ca35",
        "colab_type": "text",
        "id": "QOMG51SwLvgd"
      },
      "cell_type": "markdown",
      "source": "###Explore data distributions"
    },
    {
      "metadata": {
        "_cell_guid": "447cf76c-0ef4-4146-99d2-67b06b4b5228",
        "_uuid": "df007d73d0764d357e3c121d38726bdc943c8752",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 108
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 14092,
          "status": "ok",
          "timestamp": 1525652336480,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "bTazJagsMApY",
        "outputId": "40d2f333-1293-4fbe-bf9a-741d6a3d32db",
        "trusted": true
      },
      "cell_type": "code",
      "source": "current_learning_goal = 'CommentsClassification'\ncurrent_exclude_autosubreddits = False\ncurrent_sample_frac = 0.25\nset_columns_for_goal()\nreddit_df = get_data_for_goal()\n(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "14a1eca9-93f7-4140-aa0b-63e45d0a9a8a",
        "_uuid": "48a20a15da136e22bd49f98fb8e2105fcc00b79f",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 927
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 3777,
          "status": "ok",
          "timestamp": 1525652340270,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "IooCgeX7L0TA",
        "outputId": "7f89eeef-0a17-461e-90eb-c178d2e4291c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Reddit dataset summary:\")\n\ndisplay.display(reddit_df.describe())\n\nif current_learning_goal == \"CommentsRegression\" or current_learning_goal == \"CommentsClassification\":\n\n  plt.figure(figsize=(10,10))\n  plt.subplot(321)\n  plt.hist(reddit_df[\"NumCommenters\"],bins=20)\n  plt.yscale('log')\n  plt.title(\"NumCommenters histogram\")\n\n  plt.subplot(322)\n  plt.hist(pd.to_numeric(reddit_df[\"NumCommentersBin\"], errors='coerce'),bins='auto')\n  plt.title(\"NumCommentersBin histogram\")\n\n  plt.subplot(323)\n  plt.hist(reddit_df[\"NumComments\"],bins=20)\n  plt.yscale('log')\n  plt.title(\"NumComments histogram\")\n\n  plt.subplot(324)\n  plt.hist(pd.to_numeric(reddit_df[\"NumCommentsBin\"], errors='coerce'),bins='auto')\n  plt.title(\"NumCommentsBin histogram\")\n\n  plt.subplot(325)\n  plt.hist(reddit_df[\"Score\"],bins=20)\n  plt.yscale('log')\n  plt.title(\"Score histogram\")\n\n  plt.subplot(326)\n  plt.hist(pd.to_numeric(reddit_df[\"ScoreBin\"], errors='coerce'),bins=5)\n  plt.title(\"ScoreBin histogram\")\n\n\n  plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f9d6b3e2-1bad-4ca3-bdeb-a7ccb0a7f3d9",
        "_uuid": "69cb84cc62a919621f7f1a2593ac1a2ba7586896",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 743
        },
        "colab_type": "code",
        "collapsed": true,
        "executionInfo": {
          "elapsed": 133349,
          "status": "ok",
          "timestamp": 1525652473636,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          },
          "user_tz": 420
        },
        "id": "1lI15yICW1ei",
        "outputId": "fa797e7d-7cac-435e-bd11-e5760b265f29",
        "trusted": true
      },
      "cell_type": "code",
      "source": "current_epochs = 1\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(\n    inputs = ['Domain','Tags'], \n    outputs=['ScoreBin','NumCommentersBin'],\n    k_of_top_k_accuracy=2)\n\n(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['scorebin_output'], \n    label_classes=scorebin_classes)\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=scorebin_classes)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "29afd245-d12b-4e06-a44d-2b5551cc8e81",
        "_uuid": "1f11f594e14593f16d29f38c47012060bd568d30",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "t_wUA8WkqMLt",
        "trusted": true
      },
      "cell_type": "code",
      "source": "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(\n    inputs = ['Domain','Tags'], \n    outputs=['ScoreBin','NumCommentersBin'],\n    use_sample_weights=True,\n    k_of_top_k_accuracy=2)\n\n(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['scorebin_output'], \n    label_classes=scorebin_classes)\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=scorebin_classes)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c4cca57d-e2d2-4e08-a7b9-7d6a3fe4ca22",
        "_uuid": "00f0db89645538dc74382932946430be8a6e0a01",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "53v1oL9OOVGH",
        "trusted": true
      },
      "cell_type": "code",
      "source": "current_epochs = 1\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(\n    inputs = ['Subreddit','RedditSubmitter'], \n    outputs=['ScoreBin','NumCommentersBin'],\n    use_sample_weights=True,\n    k_of_top_k_accuracy=2)\n\n#SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n\n(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['scorebin_output'], \n    label_classes=scorebin_classes)\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=scorebin_classes)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f8894359-2440-4de1-a526-ae5bd69084da",
        "_uuid": "a042875ebf2058c80f3988651a45fc8108e4185e",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "WC6m8DOSK6gc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# How well is CM distributed for NumCommenters, when ScoreBin is used for weights\n(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['numcommentersbin_output'], \n    label_classes=scorebin_classes,\n    multi_output=True,\n    output_idx=1)\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=scorebin_classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f860b8bd-0fe4-42fb-bad9-37414f8cace2",
        "_uuid": "f0abd6d4d989a6ea916779cc79a1e8525bc454a1",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "YbwnIS3bmf1p",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Change label to NumCommentersBin and check CM composition now\n\ncurrent_epochs = 1\ncurrent_label_col = 'NumCommentersBin'\ncurrent_inverse_frequency_pow = 0.85\n\n(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n\n(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(\n    inputs = ['Subreddit','RedditSubmitter'], \n    outputs=['ScoreBin','NumCommentersBin'],\n    use_sample_weights=True,\n    k_of_top_k_accuracy=2)\n\n(y_true, y_pred) = get_true_and_predicted_labels(\n    model=model,\n    x_test=x_test, \n    y_test_selected=y_test['numcommentersbin_output'], \n    label_classes=numcommentersbin_classes,\n    multi_output=True,\n    output_idx=1)\n\nplot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, class_order=numcommentersbin_classes)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "default_view": {},
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "RedditEngagement.ipynb",
      "provenance": [],
      "version": "0.3.2",
      "views": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}